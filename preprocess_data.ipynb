{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import glob # for file handling\n",
    "\n",
    "import googlemaps\n",
    "import time\n",
    "import dotenv as dotenv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 1: Data concatenation, cleaning and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['./data\\\\Resale Flat Prices (Based on Approval Date), 1990 - 1999.csv', './data\\\\Resale Flat Prices (Based on Approval Date), 2000 - Feb 2012.csv', './data\\\\Resale Flat Prices (Based on Registration Date), From Jan 2015 to Dec 2016.csv', './data\\\\Resale Flat Prices (Based on Registration Date), From Mar 2012 to Dec 2014.csv', './data\\\\Resale flat prices based on registration date from Jan-2017 onwards.csv']\n",
      "DataFrame ./data\\Resale Flat Prices (Based on Approval Date), 1990 - 1999.csv \n",
      "Columns: Index(['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range',\n",
      "       'floor_area_sqm', 'flat_model', 'lease_commence_date', 'resale_price'],\n",
      "      dtype='object')\n",
      "DataFrame ./data\\Resale Flat Prices (Based on Approval Date), 2000 - Feb 2012.csv \n",
      "Columns: Index(['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range',\n",
      "       'floor_area_sqm', 'flat_model', 'lease_commence_date', 'resale_price'],\n",
      "      dtype='object')\n",
      "DataFrame ./data\\Resale Flat Prices (Based on Registration Date), From Jan 2015 to Dec 2016.csv \n",
      "Columns: Index(['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range',\n",
      "       'floor_area_sqm', 'flat_model', 'lease_commence_date',\n",
      "       'remaining_lease', 'resale_price'],\n",
      "      dtype='object')\n",
      "DataFrame ./data\\Resale Flat Prices (Based on Registration Date), From Mar 2012 to Dec 2014.csv \n",
      "Columns: Index(['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range',\n",
      "       'floor_area_sqm', 'flat_model', 'lease_commence_date', 'resale_price'],\n",
      "      dtype='object')\n",
      "DataFrame ./data\\Resale flat prices based on registration date from Jan-2017 onwards.csv \n",
      "Columns: Index(['month', 'town', 'flat_type', 'block', 'street_name', 'storey_range',\n",
      "       'floor_area_sqm', 'flat_model', 'lease_commence_date',\n",
      "       'remaining_lease', 'resale_price'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Preprocess initial .csv files from data.gov.sg\n",
    "csv_files = glob.glob(\"./data/*.csv\")\n",
    "print(csv_files)\n",
    "dataframes = [pd.read_csv(file) for file in csv_files]\n",
    "\n",
    "for df in dataframes:\n",
    "    df['month'] = pd.to_datetime(df['month'])\n",
    "\n",
    "# check columns of each dataframe\n",
    "for i, df in enumerate(dataframes):\n",
    "    print(f\"DataFrame {csv_files[i]} \\nColumns: {df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision to remove column 'remaining_lease' - that is onyl present for datasets post 2015\n",
    "In order to maintain consistency between all datasets, we have decided to remove the remaining_lease column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "month                  0\n",
      "town                   0\n",
      "flat_type              0\n",
      "block                  0\n",
      "street_name            0\n",
      "storey_range           0\n",
      "floor_area_sqm         0\n",
      "flat_model             0\n",
      "lease_commence_date    0\n",
      "resale_price           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Remove 'remaining_lease' column from all dataframes\n",
    "for df in dataframes:\n",
    "    if 'remaining_lease' in df.columns:\n",
    "        df.drop(columns=['remaining_lease'], inplace=True)\n",
    "\n",
    "# Create new data.csv file with all data combined\n",
    "combined_df = pd.concat(dataframes, ignore_index=True)\n",
    "combined_df.sort_values(by='month', inplace=True)\n",
    "\n",
    "# count nan values in each column\n",
    "print(combined_df.isna().sum())\n",
    "# combined_df.dropna(inplace=True) # Not required as no nan values in the dataset\n",
    "\n",
    "# due to nature of data, namely: storey range, dosent make sense to drop duplicates\n",
    "# duplicates = combined_df[combined_df.duplicated()]\n",
    "# print(f\"Number of duplicate rows: {duplicates.shape[0]}\")\n",
    "\n",
    "# # remove duplicates\n",
    "# combined_df.drop_duplicates(inplace=True)\n",
    "\n",
    "# # check for duplicates again\n",
    "# duplicates = combined_df[combined_df.duplicated()]\n",
    "# print(f\"Number of duplicate rows after removal: {duplicates.shape[0]}\")\n",
    "\n",
    "# reset index\n",
    "combined_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# final csv file\n",
    "# combined_df.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Section 2: Feature Engineering: Calculating distance of HDB from MRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     block        street_name  counts\n",
      "0        1           BEACH RD     147\n",
      "1        1    BEDOK STH AVE 1     228\n",
      "2        1       CHAI CHEE RD     129\n",
      "3        1  CHANGI VILLAGE RD      35\n",
      "4        1          DELTA AVE      89\n",
      "...    ...                ...     ...\n",
      "9885   99A    LOR 2 TOA PAYOH      50\n",
      "9886   99B    LOR 2 TOA PAYOH      43\n",
      "9887   99C    LOR 2 TOA PAYOH      52\n",
      "9888    9A      BOON TIONG RD      32\n",
      "9889    9B      BOON TIONG RD      26\n",
      "\n",
      "[9890 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "# Using google maps api, get lat long coordinates of each location, add to dataframe\n",
    "\n",
    "''' \n",
    "Rate Limit: \n",
    "3,000 QPM (queries per minute), calculated as the sum of client-side and server-side queries.\n",
    "\n",
    "We have about 950k rows in our dataset, one way to solve would be to cache similar results\n",
    "'''\n",
    "\n",
    "# calculating number of similar blocks + street names to cache\n",
    "count_pairs = combined_df.groupby(['block', 'street_name']).size().reset_index(name='counts')\n",
    "print(count_pairs)\n",
    "\n",
    "# drop 'counts' column\n",
    "count_pairs.drop(columns=['counts'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Since we have only 10k unique combinations, we can just add a sleep (hopefully i dont go broke)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'address_components': [{'long_name': '10', 'short_name': '10', 'types': ['street_number']}, {'long_name': 'Yishun Avenue 9', 'short_name': 'Yishun Ave 9', 'types': ['route']}, {'long_name': 'Yishun', 'short_name': 'Yishun', 'types': ['neighborhood', 'political']}, {'long_name': 'Singapore', 'short_name': 'Singapore', 'types': ['locality', 'political']}, {'long_name': 'Singapore', 'short_name': 'SG', 'types': ['country', 'political']}, {'long_name': '768888', 'short_name': '768888', 'types': ['postal_code']}], 'formatted_address': '10 Yishun Ave 9, Singapore 768888', 'geometry': {'bounds': {'northeast': {'lat': 1.4318163, 'lng': 103.8400468}, 'southwest': {'lat': 1.43143, 'lng': 103.8396539}}, 'location': {'lat': 1.4316347, 'lng': 103.8398656}, 'location_type': 'ROOFTOP', 'viewport': {'northeast': {'lat': 1.432972130291502, 'lng': 103.8411993302915}, 'southwest': {'lat': 1.430274169708498, 'lng': 103.8385013697085}}}, 'navigation_points': [{'location': {'latitude': 1.4317223, 'longitude': 103.8397856}}], 'place_id': 'ChIJmZsZZWYU2jER2OoeNNOfD5Q', 'types': ['premise', 'street_address']}]\n",
      "{'lat': 1.4316347, 'lng': 103.8398656}\n"
     ]
    }
   ],
   "source": [
    "gmaps = googlemaps.Client(key=dotenv.get_key('.env', 'GOOGLE_MAPS_API_KEY'))\n",
    "\n",
    "geocode_cache = {}\n",
    "\n",
    "# test api call so i dont go broke\n",
    "response = gmaps.geocode(\"10 Yishun Ave 9, Singapore\")\n",
    "print(response)\n",
    "print(response[0]['geometry']['location'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 2500 addresses. Sleeping 90s for API limit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 30\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m2500\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     29\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcount\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m addresses. Sleeping 90s for API limit\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 30\u001b[0m         time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m90\u001b[39m)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Convert cache dictionary to a DataFrame\u001b[39;00m\n\u001b[0;32m     33\u001b[0m cache_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(\n\u001b[0;32m     34\u001b[0m     [(k[\u001b[38;5;241m0\u001b[39m], k[\u001b[38;5;241m1\u001b[39m], v[\u001b[38;5;241m0\u001b[39m], v[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m geocode_cache\u001b[38;5;241m.\u001b[39mitems()],\n\u001b[0;32m     35\u001b[0m     columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstreet_name\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlatitude\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlongitude\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     36\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def geocode_address(block, street):\n",
    "    address = f\"Block {block}, {street}, Singapore\"\n",
    "    try:\n",
    "        geocode_result = gmaps.geocode(address)\n",
    "        if geocode_result:\n",
    "            location = geocode_result[0]['geometry']['location']\n",
    "            return location['lat'], location['lng']\n",
    "    except Exception as e:\n",
    "        print(f\"Error geocoding {address}: {e}\")\n",
    "    return None, None\n",
    "\n",
    "# Loop through the DataFrame and geocode each unique address\n",
    "count = 0\n",
    "for idx, row in count_pairs.iterrows():\n",
    "    block = row['block']\n",
    "    street = row['street_name']\n",
    "    \n",
    "    # Skip if already cached\n",
    "    if (block, street) in geocode_cache:\n",
    "        continue\n",
    "    \n",
    "    # Call Geocoding API\n",
    "    lat, lng = geocode_address(block, street)\n",
    "    geocode_cache[(block, street)] = (lat, lng)\n",
    "    print(f\"Geocoded {block}, {street}: {lat}, {lng}\")\n",
    "    \n",
    "    # Sleep every 2500 requests for 90s\n",
    "    count += 1\n",
    "    if count % 2500 == 0:\n",
    "        print(f\"Processed {count} addresses. Sleeping 90s for API limit\")\n",
    "        time.sleep(90)\n",
    "\n",
    "# Convert cache dictionary to a DataFrame\n",
    "cache_df = pd.DataFrame(\n",
    "    [(k[0], k[1], v[0], v[1]) for k, v in geocode_cache.items()],\n",
    "    columns=['block', 'street_name', 'latitude', 'longitude']\n",
    ")\n",
    "\n",
    "df_merged = combined_df.merge(cache_df, on=['block', 'street_name'], how='left')\n",
    "\n",
    "# Save the result\n",
    "df_merged.to_csv(\"data_with_latlong.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
